import json
from collections import defaultdict
from typing import List

from dotenv import load_dotenv
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper
from langchain_core.messages import BaseMessage, ToolMessage, HumanMessage, AIMessage
# the tool message class is one interface that represents a result of a tool execution that we want to downstream to the LLM eventually.

from langgraph.prebuilt import ToolInvocation, ToolExecutor

from chains import parser
from schemas import AnswerQuestion, Reflection

load_dotenv()

search = TavilySearchAPIWrapper() # a wrapper over Tavily API
tavily_tool = TavilySearchResults(api_wrapper=search, max_results=5)  # convert the wrapper into a langchain tool
tool_executor = ToolExecutor([tavily_tool])
# Because we're going to have many invocations of the Tavily API. And we want to run them in parallel. We don't want to run them synchronously. So the tool executor has a batch method which takes all the tool invocations that we gather together in tool_invocations list. And the tool executor with the batch function simply executes all the tool invocations with a thread pool. It creates, running everything in parallel so everything would run much faster.


# this function is going to receive the state which is simply a list of messages. It's going to extract the tools that needs to be executed from those messages. It's going to execute them, and it's going to return a list of tool messages. And this function is going to be running the tavily search tool for our search queries that we generated in the previous step.
def execute_tools(state: List[BaseMessage]) -> List[ToolMessage]:
    tool_invocation: AIMessage = state[-1]
    parsed_tool_calls = parser.invoke(tool_invocation) # parse out the tool invocation generated from the LLM
    ids = []
    tool_invocations = [] # we're going to populate tool invocations with langchain elements which are objects with the information of which tool to use, and which function (in our code) and which inputs to call it with.

    for parsed_call in parsed_tool_calls:   # in our case, there is just one tool_call ie. tavily, so size of parsed_tool_calls = 1
        for query in parsed_call["args"]["search_queries"]:
            tool_invocations.append(
                ToolInvocation(
                    tool="tavily_search_results_json",
                    tool_input=query,
                )
            )
            ids.append(parsed_call["id"]) # So this is the ID that originally was from the function call for the answer question object. And we want to append it to the IDs list. So we can correlate this tool invocation that where did it come from.

    outputs = tool_executor.batch(tool_invocations)
    # OUTPUT: And we can see we have for each search query we have 5 different results because we have put max result equals 5. And every answer from the travel search engine comes in the form of the URL,which tells where did it originate from, and the content, which is a short summary of the URLs content.

    # the last thing we need to do in this function is simply to take this output, and transform it into a list of tool messages where we're going to have one tool message. And this tool message is going to have and include the three searches that we had for each search query, where each search is going to have five different results, to add into the state of the graph.

    # Map each output to its corresponding ID and tool input
    outputs_map = defaultdict(dict)
    for id_, output, invocation in zip(ids, outputs, tool_invocations):
        outputs_map[id_][invocation.tool_input] = output

    # Convert the mapped outputs to ToolMessage objects
    tool_messages = []
    for id_, mapped_output in outputs_map.items():
        tool_messages.append(
            ToolMessage(content=json.dumps(mapped_output), tool_call_id=id_)
        )

    return tool_messages


if __name__ == "__main__":
    print("Tool Executor Enter")

    # for testing
    human_message = HumanMessage(
        content="Write about AI-Powered SOC / autonomous soc  problem domain,"
        " list startups that do that and raised capital."
    )

    answer = AnswerQuestion(
        answer="",
        reflection=Reflection(missing="", superfluous=""),
        search_queries=[
            "AI-powered SOC startups funding",
            "AI SOC problem domain specifics",
            "Technologies used by AI-powered SOC startups",
        ],
        id="call_KpYHichFFEmLitHFvFhKy1Ra", # auto-generated by LLM which have function calling to correlate between the two execution result with the originated function calling request.
    )

    raw_res = execute_tools(
        state=[
            human_message,
            AIMessage(
                content="",
                tool_calls=[
                    {
                        "name": AnswerQuestion.__name__,
                        "args": answer.dict(),
                        # answer_question object: Previously, we output parsed the string reponse of LLM into a pedantic object. Now we're going to reverse it. So we want to put it as in its raw format.  
                        "id": "call_KpYHichFFEmLitHFvFhKy1Ra",
                    }
                ],
            ),
        ]
    )
    print(raw_res)
